{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "    Gather\n",
    "    Asses\n",
    "    Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Definition and An Analogy\n",
    "#### A Definition\n",
    "    Wrangling is a weird word. Let’s check the definition. This is exactly what I did when I first heard the term and was perplexed just as you may be right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So wrangling means to round up, herd, or take charge of livestock, like horses or sheep. Let's focus in on the sheep example.\n",
    "\n",
    "    A shepherd's main goals are to get their sheep to their pastures to let them graze, guide them to market to shear them, and put them in the barn to sleep. Before any of that though, they must be rounded up in a nice and organized group. The consequences if they're not? These tasks take longer. If they're all scattered, some could also run off and get lost. A wolf could even sneak into the pack and feast on a few of them.\n",
    "\n",
    "#### An Analogy\n",
    "    The same idea of organizing before acting is true for those who are shepherds of data. We need to wrangle our data for good outcomes, otherwise there could be consequences. If we analyze, visualize, or model our data before we wrangle it, our consequences could be making mistakes, missing out on cool insights, and wasting time. So best practices say wrangle. Always.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Click-through_rate\n",
    "\n",
    "https://charliepark.org/slopegraphs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap Data from \n",
    "\n",
    "https://www.kaggle.com/udacity/armenian-online-job-postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather: Download\n",
    "    The dataset used in this lesson is hosted on this Kaggle Datasets page: Armenian Online Job Postings. Some context on this dataset, from the description section of that page:\n",
    "\n",
    "    The online job market is a good indicator of overall demand for labor in an economy. This dataset consists of 19,000 job postings from 2004 to 2015 posted on CareerCenter, an Armenian human resource portal.\n",
    "\n",
    "    Since postings are text documents and tend to have similar structures, text mining can be used to extract features like posting date, job title, company name, job description, salary, and more. Postings that had no structure or were not job-related were removed. The data was originally scraped from a Yahoo! mailing group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practice: Downloading Files Programmatically\n",
    "    When downloading files from the internet, downloading can be done manually by clicking the download button (or sometimes right-clicking on a link and clicking \"Save file as\"). But best practice is actually to download files programmatically, i.e. with code, for two reasons: scalability and reproducibility.\n",
    "\n",
    "#### Scalability: \n",
    "    Imagine you had a thousand files to download on a thousand different web pages, instead of just one. It'd take an eternity to point and click a thousand times. You can do the same with a few lines of code.\n",
    "\n",
    "#### Reproducibility: \n",
    "    Someone, whether it's you or another person, is likely going to want to run your analysis later, so make downloading the dataset or datasets as easy on that person as possible. Reproducibility is also one of the main principles of the scientific method[https://en.wikipedia.org/wiki/Scientific_method#Documentation_and_replication]. You want to be able to prove to people that your analysis, visualization, etc. is legitimate. People need to know that given your data, your computational environment, your code, etc., that they can reproduce your results! Plus, the dataset or the web page it lives on may change, so if you include the date you downloaded the dataset, you give these future onlookers a chance to access archived copies of the dataset or at least understand why their results are different.\n",
    "    \n",
    "scientific method [https://en.wikipedia.org/wiki/Scientific_method#Documentation_and_replication]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnZip files using python\n",
    "https://docs.python.org/3/library/zipfile.html#zipfile-objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"data/armenian-online-job-postings.zip\") as fp:\n",
    "        fp.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is a zip file ?\n",
    "https://www.lifewire.com/zip-file-2622675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is a contaxt managers ? \n",
    "https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "\n",
    "Low quality data is commonly referred to as dirty data. Dirty data has issues with its content.\n",
    "\n",
    "Imagine you had a table with two columns: Name and Height, like below:\n",
    "\n",
    "A table with Name and Height headers\n",
    "\n",
    "<img src=\"data1.png\" height=200 weight=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common data quality issues include:\n",
    "\n",
    "* missing data, like the missing height value for Juan.\n",
    "* invalid data, like a cell having an impossible value, e.g., like negative height value for Kwasi. Having \"inches\" and \"centimetres\" in the height entries is technically invalid as well, since the datatype for height becomes a string when those are present. The datatype for height should be integer or float.\n",
    "* inaccurate data, like Jane actually being 58 inches tall, not 55 inches tall.\n",
    "* inconsistent data, like using different units for height (inches and centimetres).\n",
    "\n",
    "\n",
    "    Data quality is a perception or an assessment of data's fitness to serve its purpose in a given context. Unfortunately, that’s a bit of an evasive definition but it gets to something important: there are no hard and fast rules for data quality. One dataset may be high enough quality for one application but not for another.\n",
    "\n",
    "#### Tidiness\n",
    "\n",
    "    Untidy data is commonly referred to as \"messy\" data. Messy data has issues with its structure.\n",
    "\n",
    "Tidy data is a relatively new concept coined by statistician, professor, and all-round data expert <a href=\"http://hadley.nz/\">Hadley Wickham</a>. I’m going to take a quote from his excellent <a href=\"https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html\">paper</a> on the subject:\n",
    "\n",
    "    It is often said that 80% of data analysis is spent on the cleaning and preparing data. And it’s not just a first step, but it must be repeated many times over the course of analysis as new problems come to light or new data is collected. To get a handle on the problem, this paper focuses on a small, but important, aspect of data cleaning that I call data tidying: structuring datasets to facilitate analysis.\n",
    "\n",
    "...\n",
    "\n",
    "A dataset is messy or tidy depending on how rows, columns, and tables are matched up with observations, variables, and types. In tidy data:\n",
    "\n",
    "* Each variable forms a column.\n",
    "* Each observation forms a row.\n",
    "* Each type of observational unit forms a table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data in Python\n",
    "http://www.jeannicholashould.com/tidy-data-in-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visual Assessment\n",
    "\n",
    "    Visual assessment is simple. Open your data in your favorite software application (Google Sheets, Excel, a text editor, etc.) and scroll through it, looking for quality and tidiness issues.\n",
    "\n",
    "<img src=\"data3.png\" height=400 width=400>\n",
    "\n",
    "#### Programmatic Assessment\n",
    "\n",
    "    Programmatic assessment tends to be more efficient than visual assessment. One simple example of a programmatic assessment is pandas' info method, which gives us the basic info of your DataFrame—like number of entries, number of columns, the types of each column, whether there are missing values, and more.\n",
    "\n",
    "<img src=\"data4.png\" height=400 width=400>\n",
    "\n",
    "\n",
    "\n",
    "other example is using pandas' plotting capabilities through the plot method, though simple visualizations are more common in exploratory data analysis (we'll discuss this later in this lesson) rather than data wrangling.\n",
    "\n",
    "These types of assessments are handy for gauging your data’s structure and also for quickly spotting things that we’ll need to clean.\n",
    "\n",
    "\n",
    "<img src=\"data5.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escape character\n",
    "https://en.wikipedia.org/wiki/Escape_character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Common Programmatic Assessments\n",
    "    Now it's time to explore programmatic assessments for yourself! Again, this is where we use code to help detect problems in our data that aren’t as easily detectable with the human eye.\n",
    "\n",
    "The data wrangling template is displayed in the Jupyter Notebook below with empty cells for four common programmatic assessment methods in pandas (documention pages linked below):\n",
    "\n",
    "* head\n",
    "* tail\n",
    "* info\n",
    "* value_counts\n",
    "\n",
    "Execute these assessment as per the instructions in those cells. In the following quizzes, you'll be asked to replicate these statements.\n",
    "\n",
    "For these quizzes and all quizzes going forward, don’t go into them just trying to get them right. Exploring is a key part of learning. Get the code right, then I encourage you to explore the documentation, try various parameters, try new things and see where things break. Error messages are your friend, because you can learn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://simplystatistics.org/2016/02/17/non-tidy-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean: Intro\n",
    "\n",
    "#### Improving Quality and Tidiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Cleaning means acting on the assessments we made to improve quality and tidiness.\n",
    "\n",
    "\n",
    "#### Improving Quality\n",
    "\n",
    "#### Improving quality doesn’t mean changing the data to make it say something different—that's data fraud.\n",
    "\n",
    "Consider the animals DataFrame, which has headers for name, body weight (in kilograms), and brain weight (in grams). The last five rows of this DataFrame are displayed below:\n",
    "\n",
    "<img src='data6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of improving quality include:\n",
    "\n",
    "* Correcting when inaccurate, like correcting the mouse's body weight to 0.023 kg instead of 230 kg\n",
    "* Removing when irrelevant, like removing the row with \"Apple\" since an apple is a fruit and not an animal\n",
    "* Replacing when missing, like filling in the missing value for brain weight for Brachiosaurus\n",
    "* Combining, like concatenating the missing rows in the more_animals DataFrame displayed below\n",
    "\n",
    "<img src='data7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Tidiness\n",
    "\n",
    "    Improving tidiness means transforming the dataset so that each variable is a column, each observation is a row, and each type of observational unit is a table. There are special functions in pandas that help us do that.\n",
    "    \n",
    "### Programmatic Data Cleaning Process\n",
    "\n",
    "The programmatic data cleaning process:\n",
    "\n",
    "* Define\n",
    "* Code\n",
    "* Test\n",
    "    \n",
    "<b>Defining</b> means defining a data cleaning plan in writing, where we turn our assessments into defined cleaning tasks. This plan will also serve as an instruction list so others (or us in the future) can look at our work and reproduce it.\n",
    "    \n",
    "<b>Coding</b> means translating these definitions to code and executing that code.\n",
    "\n",
    "<b>Testing</b> means testing our dataset, often using code, to make sure our cleaning operations worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining, then Coding and Testing Immediately\n",
    "For pedagogical purposes in this lesson, we will be performing the define, code, and test steps of cleaning data programmatically in order. In other words, we write all of the definitions, then convert all of the definitions to code, then test all of the cleaning operations.\n",
    "\n",
    "In reality, it is often more practical to define a cleaning operation, then immediately code and test it. The data wrangling template still applies here, except you'll have multiple Define, Code, and Test subheadings, with third level headers (###) denoting each issue, as displayed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing and Subsetting DataFrames in Python\n",
    "https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://wiki.python.org/moin/ForLoop\"> python loops </a>\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html\">Pandas: Options and Settings </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling vs. EDA\n",
    "\n",
    "    Here is one definition of EDA: an analysis approach that focuses on identifying general patterns in the data, and identifying outliers and features of the data that might not have been anticipated.\n",
    "\n",
    "So where does data wrangling end and EDA start?\n",
    "\n",
    "    Data wrangling is about gathering the right pieces of data, assessing your data's quality and structure, then modifying your data to make it clean. But the assessments you make and convert to cleaning operations won't make your analysis, viz, or model better, though. The goal is to just make them possible, i.e., functional.\n",
    "\n",
    "    EDA is about exploring your data to later augment it to maximize the potential of our analyses, visualizations, and models. When exploring, simple visualizations are often used to summarize your data's main characteristics. From there you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering. Or detect and remove outliers so your model's fit is better.\n",
    "\n",
    "In practice, wrangling and EDA can and often do occur together, but we're going to separate them for teaching purposes.\n",
    "\n",
    "### ETL\n",
    "\n",
    "You also may have heard of the extract-transform-load process also known as ETL. ETL differs from data wrangling in three main ways:\n",
    "\n",
    "* The users are different\n",
    "* The data is different\n",
    "* The use cases are different\n",
    "\n",
    "This article (Data Wrangling Versus ETL: What’s the Difference?) by Wei Zhang explains these three differences well.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_engineering\n",
    "\n",
    "https://tdwi.org/articles/2017/02/10/data-wrangling-and-etl-differences.aspx\n",
    "\n",
    "https://en.wikipedia.org/wiki/Extract,_transform,_load"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Data Wrangling Summary\n",
    "\n",
    "\n",
    "\n",
    "#### Gather\n",
    "\n",
    "* Depending on the source of your data, and what format it's in, the steps in gathering data vary.\n",
    "* High-level gathering process: obtaining data (downloading a file from the internet, scraping a web page, querying an API, etc.) and importing that data into your programming environment (e.g., Jupyter Notebook).\n",
    "\n",
    "#### Assess\n",
    "\n",
    "Assess data for:\n",
    "\n",
    "* Quality: issues with content. Low quality data is also known as dirty data.\n",
    "* Tidiness: issues with structure that prevent easy analysis. Untidy data is also known as messy data. Tidy data requirements:\n",
    "    Each variable forms a column.\n",
    "    Each observation forms a row.\n",
    "    Each type of observational unit forms a table.\n",
    "\n",
    "Types of assessment:\n",
    "\n",
    "* Visual assessment: scrolling through the data in your preferred software application (Google Sheets, Excel, a text editor, etc.).\n",
    "* Programmatic assessment: using code to view specific portions and summaries of the data (pandas' head, tail, and info methods, for example).\n",
    "\n",
    "\n",
    "#### Clean\n",
    "\n",
    "Types of cleaning:\n",
    "\n",
    "* Manual (not recommended unless the issues are single occurrences)\n",
    "* Programmatic\n",
    "\n",
    "The programmatic data cleaning process:\n",
    "* Define: convert our assessments into defined cleaning tasks. These definitions also serve as an instruction list so others (or yourself in the future) can look at your work and reproduce it.\n",
    "* Code: convert those definitions to code and run that code.\n",
    "* Test: test your dataset, visually or with code, to make sure your cleaning operations worked.\n",
    "\n",
    "Always make copies of the original pieces of data before cleaning!\n",
    "\n",
    "#### Reassess and Iterate\n",
    "* After cleaning, always reassess and iterate on any of the data wrangling steps if necessary.\n",
    "\n",
    "#### Store (Optional)\n",
    "* Store data, in a file or database for example, if you need to use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
