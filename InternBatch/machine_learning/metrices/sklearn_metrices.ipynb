{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let us look at the following **confusion matrix**. What is the accuracy for the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix \n",
    "\n",
    "<img src=\"Confusion_matrix.jpeg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very easily, you will notice that the accuracy for this model is very very high, at 99.9%!! Wow! You have hit the jackpot and holy grail (*scream and run around the room, pumping the fist in the air several times*)!\n",
    "\n",
    "\n",
    "But….(well you know this is coming right?) what if I mentioned that the positive over here is actually someone who is sick and carrying a virus that can spread very quickly? Or the positive here represent a fraud case? Or the positive here represents terrorist that the model says its a non-terrorist? Well you get the idea. The costs of having a mis-classified actual positive (or false negative) is very high here in these three circumstances that I posed.\n",
    "\n",
    "OK, so now you realized that accuracy is not the be-all and end-all model metric to use when selecting the best model…now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "So if you look at Wikipedia, you will see that the the formula for calculating Precision and Recall is as follows:\n",
    "\n",
    "Let me put it here for further explanation.\n",
    "\n",
    "<img src=\"precision_recall.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me put in the confusion matrix and its parts here.\n",
    "\n",
    "<img src=\"cmatrix.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision\n",
    "\n",
    "Great! Now let us look at Precision first.\n",
    "\n",
    "<img src=\"precsion.png\">\n",
    "\n",
    "    What do you notice for the denominator? The denominator is actually the Total Predicted Positive! So the formula becomes\n",
    "   \n",
    "<img src='cnf_matrix.png'>\n",
    "\n",
    "$$ \\text {True Positive + False Positive = Total Predicted Positive} $$\n",
    "\n",
    "<img src=\"prcision.png\">\n",
    "\n",
    "\n",
    "#### Immediately, you can see that Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.\n",
    "\n",
    "\n",
    " **Precision** is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.\n",
    " \n",
    " \n",
    "### Recall\n",
    "\n",
    "    So let us apply the same logic for Recall. Recall how Recall is calculated.\n",
    "    \n",
    "<img src=\"recall.png\">\n",
    "\n",
    "<img src=\"recall_cf.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! So **Recall** actually calculates how many of the Actual Positives our model capture through labeling it as **Positive (True Positive)**. Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with **False Negative**.\n",
    "\n",
    "    For instance, in fraud detection or sick patient detection. If a fraudulent transaction (Actual Positive) is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n",
    "\n",
    "    Similarly, in sick patient detection. If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.\n",
    "    \n",
    "       recall is also known as sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "    \n",
    "    F1 Score is harmonic mean of Precision and Recall\n",
    "\n",
    "Now if you read a lot of other literature on Precision and Recall, you cannot avoid the other measure, F1 which is a function of Precision and Recall. Looking at Wikipedia, the formula is as follows:\n",
    "\n",
    "$$ \\text { F1 } = 2 X \\frac {Precision*Recall} {Precision+Recall} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    F1 Score is needed when you want to seek a balance between Precision and Recall. Right…so what is the difference between F1 Score and Accuracy then? We have previously seen that accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
